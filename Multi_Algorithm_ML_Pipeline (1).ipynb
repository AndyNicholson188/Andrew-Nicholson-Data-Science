{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a97673-4d8f-4531-99c8-b1dc761fc492",
   "metadata": {},
   "source": [
    "## Project Overview: Multi-Algorithm ML Pipeline\n",
    "\n",
    "This notebook benchmarks several machine learning algorithms within a unified pipeline structure. It evaluates model performance under consistent preprocessing, validation, and metric reporting workflows.\n",
    "\n",
    "Key aspects:\n",
    "- Hyperparameter tuning across Logistic Regression, Decision Trees, SVMs, Gradient Boosting and Neural Network models.\n",
    "- Tracks key metrics: accuracy, precision, recall, F1, AUC.\n",
    "- Outputs SHAP values for feature importance exploration.\n",
    "- Built for extensibility and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afd118-8580-44cc-acb9-c820316c5a97",
   "metadata": {},
   "source": [
    "## SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86859479-b3a6-42e5-978d-c1a9eed50a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bf986-9873-43d8-82a4-990eca9bf82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af81223-da74-4714-9f11-492aae6b3394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eac632-1a9c-48c9-8cfa-bdd8bc639e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceec18b-edec-4d98-ae71-cf5324868973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "import pipeline\n",
    "import importlib\n",
    "importlib.reload(pipeline)\n",
    "import joblib\n",
    "import tarfile\n",
    "import gc # garbage collection (reclaim memory marked for deletion)\n",
    "import math\n",
    "import datetime\n",
    "import sklearn\n",
    "import fnmatch\n",
    "#import lightgbm as lgb - can't use because of problem with dependency and Pandas. Investigate later.\n",
    "import catboost as cb\n",
    "import model_pipeline\n",
    "importlib.reload(model_pipeline)\n",
    "import logging\n",
    "import shap\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, roc_curve, auc, classification_report, roc_auc_score, precision_score as sk_precision_score, \n",
    "    recall_score, f1_score, log_loss, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from io import BytesIO\n",
    "from scipy import sparse \n",
    "from scipy.sparse import save_npz\n",
    "from scipy.stats import randint, uniform\n",
    "from typing import List, Tuple, Optional\n",
    "from itertools import product\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348999c2-faee-4fb1-b9aa-a00b39de6e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialise SageMaker\n",
    "sagemaker_session = None  # Removed for public version\n",
    "role = get_execution_role()\n",
    "\n",
    "# Global variables\n",
    "client_name = \"Generic_Client\"\n",
    "training_task_name = \"Generic_Client_Cloud\"\n",
    "target_column_name = \"MODEL_TARGET\"\n",
    "partition_column_name = \"TVH_PARTITION\"\n",
    "dataset_filename = \"Generic_Client_Latest_DC.csv\" # Update with the actual path\n",
    "database_schema = \"\"\n",
    "database_feature_table = \"\"\n",
    "use_rodbc = False \n",
    "problem_type = \"binary_classification\"\n",
    "bEnsemble = False\n",
    "ENQ_KEY_VAR = \"IDENTIFIER\"\n",
    "rand_runs = 1\n",
    "high_card_cut_off = 100\n",
    "ds_ratio = 3\n",
    "positive_target = 1\n",
    "negative_target = 0\n",
    "\n",
    "# SageMaker-specific variables\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = 'Generic_Bucket' # Replace with your S3 bucket name\n",
    "prefix = 'Generic_Client' # Replace with the desired S3 prefix\n",
    "\n",
    "# Setting up paths\n",
    "train_data_s3_path = f's3://{bucket}/{prefix}/train_Generic_Client.csv'\n",
    "validation_data_s3_path = f's3://{bucket}/{prefix}/validation_Generic_Client.csv'\n",
    "test_data_s3_path = f's3://{bucket}/{prefix}/test_Generic_Client.csv'\n",
    "model_output_s3_path = f's3://{bucket}/{prefix}/model_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267d454-6245-4a4d-a298-f954b197ee0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b785c33-8f5f-4c41-b55a-d8638da24c04",
   "metadata": {},
   "source": [
    "### Read from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec96001-a3a0-40ad-96d3-5be2b065465a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the S3 URL for the file\n",
    "s3_uri = f's3://{bucket}/{prefix}/{dataset_filename}'\n",
    "\n",
    "# Read data from S3 into a pandas DataFrame\n",
    "df = pd.read_csv(s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a9a23-6d37-4e24-b59a-d6a4d1f7c984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127f0df-19b8-406c-987a-77e00590bb10",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59be91-9aad-45f7-a285-c2e205c42df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187bf5ec-e38f-4d22-af2b-c3d00b5b1674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab40340-c759-4b3e-a0be-0b401ea0b0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Return tuple (number of rows, number of columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df1b21-faff-4b8a-b14a-44b72785054b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform a full EDA\n",
    "pipeline.perform_eda(df, save_plots=True)\n",
    "\n",
    "# Or, use individual functions for specific analyses\n",
    "print(pipeline.summarize_data(data))\n",
    "pipeline.plot_histograms(data, save_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba45ab-7aec-4534-842c-f15d342c0f0c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d845ae-46ef-4a59-a4a6-1a0c5c54eb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove all columns not to be used for modelling, except for IDENTIFIER and TVH_PARTITION, which will be removed below.\n",
    "df2 = df.drop([\"column_1\", \"column_2\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739cf879-210a-4f88-b7dd-37f5ce10d16c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47cdaa-35a8-478f-9d5d-f8da3c2712f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de787d6-cbf3-442c-af93-a80442cabffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove columns with only one unique value (including nulls)\n",
    "df2 = df2.loc[:, df.nunique(dropna=False) > 1]\n",
    "print(\"Columns with only one unique value:\\n\", df.columns[df.nunique(dropna=False) == 1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b205f-8b4f-4c5d-bade-b116acb0b31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert all DataFrame column names to upper case\n",
    "df2.columns = [col.upper() for col in df2.columns]\n",
    "\n",
    "# Check for model_target and tvh_partition in df. If absent, raise error\n",
    "required_columns = ['MODEL_TARGET', 'TVH_PARTITION'] # TVH_PARTITION = train (value of 1), validation (value of 2) and holdout ((value of 3)) partitions.\n",
    "\n",
    "# Check if required columns are in the DataFrame\n",
    "if not all(col in df2.columns for col in required_columns):\n",
    "    missing_cols = [col for col in required_columns if col not in df2.columns]\n",
    "    raise ValueError(f\"Missing required columns: {missing_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2556eb48-e771-4491-bbe0-5933bfa16683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If MODEL_TARGET contains any values other than 0 or 1, raise an error.\n",
    "if df2['MODEL_TARGET'].isin([0, 1]).all() == False:\n",
    "    raise ValueError(\"MODEL_TARGET contains invalid values (not 0 or 1).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6861f1c8-11b4-4805-beed-080aead83104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 'MODEL_TARGET' and 'TVH_PARTITION' columns to integer\n",
    "df2['MODEL_TARGET'] = df2['MODEL_TARGET'].astype(int)\n",
    "df2['TVH_PARTITION'] = df2['TVH_PARTITION'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fbac1b-61d7-4e2b-86fb-5e419ee55bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_high_card, high_card_cols = pipeline.drop_high_cardinality_columns(df2, max_unique_values=100)\n",
    "print(\"High cardinality columns: \\n\", high_card_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89338bc-4d42-4e91-8977-124689f05539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_high_card.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4b6c5-e1c1-4e2e-a8c3-43c3efa587e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into train, validation and test sets\n",
    "train_df = df_high_card[df_high_card['TVH_PARTITION'] == 1]\n",
    "validation_df = df_high_card[df_high_card['TVH_PARTITION'] == 2]\n",
    "test_df = df_high_card[df_high_card['TVH_PARTITION'] == 3] # ADD value of 4 in future update!\n",
    "\n",
    "print(\"Training set shape (rows, columns):\", train_df.shape)\n",
    "print(\"Validation set shape (rows, columns):\", validation_df.shape)\n",
    "print(\"Test set shape (rows, columns):\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65acce3b-e563-46fa-992a-bf1531689256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_high_card[df_high_card['TVH_PARTITION'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a9236-c03c-4c3a-ba3d-82c86850eb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_undersampled = pipeline.undersample_classification_task(train_df, ds_ratio)\n",
    "print(\"Shape of undersampled training set (rows, columns): \", train_undersampled.shape)\n",
    "\n",
    "# Number of columns in undersampled training set must be the same as in the original training set\n",
    "assert train_undersampled.shape[1] == train_df.shape[1], f\"Number of columns must be the same: {train_undersampled.shape[1]}, {train_df.shape[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9219f0a3-9ec8-4b75-83f9-74f56f467466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define local file paths\n",
    "train_path = f'/home/sagemaker-user/Product/train_Generic_Client_DS_{ds_ratio}.csv'\n",
    "validation_path = f'/home/sagemaker-user/Product/validation_Generic_Client_DS_{ds_ratio}.csv'\n",
    "test_path = f'/home/sagemaker-user/Product/test_Generic_Client_DS_{ds_ratio}.csv'\n",
    "\n",
    "train_identifiers_path = f'/home/sagemaker-user/Product/train_identifiers_Generic_Client_DS_{ds_ratio}.csv'\n",
    "validation_identifiers_path = f'/home/sagemaker-user/Product/validation_identifiers_Generic_Client_DS_{ds_ratio}.csv'\n",
    "test_identifiers_path = f'/home/sagemaker-user/Product/test_identifiers_Generic_Client_DS_{ds_ratio}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e20d6a-bcb4-4244-b6ae-68551d30a31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save feature data for modelling (excluding IDENTIFIER and TVH_PARTITION)\n",
    "# Shuffle training - randomly sample all rows in the undersampled training set, then reset the index, then drop the original index, \n",
    "# otherwise it will be a separate column in each dataset.\n",
    "train_undersampled_shuffled = train_undersampled.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_undersampled_shuffled.drop(['IDENTIFIER', 'TVH_PARTITION'], axis=1).to_csv(train_path, index=False)\n",
    "validation_df.drop(['IDENTIFIER', 'TVH_PARTITION'], axis=1).to_csv(validation_path, index=False)\n",
    "test_df.drop(['IDENTIFIER', 'TVH_PARTITION'], axis=1).to_csv(test_path, index=False)\n",
    "\n",
    "# Save only the identifiers and target variable for later use\n",
    "train_identifiers = train_undersampled_shuffled[['IDENTIFIER', 'TVH_PARTITION', 'MODEL_TARGET']]\n",
    "validation_identifiers = validation_df[['IDENTIFIER', 'TVH_PARTITION', 'MODEL_TARGET']]\n",
    "test_identifiers = test_df[['IDENTIFIER', 'TVH_PARTITION', 'MODEL_TARGET']]\n",
    "\n",
    "# Choose appropriate paths for saving the identifiers\n",
    "train_identifiers.to_csv(train_identifiers_path, index=False)\n",
    "validation_identifiers.to_csv(validation_identifiers_path, index=False)\n",
    "test_identifiers.to_csv(test_identifiers_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61342b9-57e4-4c27-b524-0e15f2a4b1f7",
   "metadata": {},
   "source": [
    "#### OPTIONAL (run if you want to load train, val and test without first running all previous steps, except for installs, imports and instantiation of global variables.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3266f-011f-4861-a456-14b7619c4666",
   "metadata": {},
   "source": [
    "##### Run if you want to load downsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb84983-1feb-4ee2-890d-ad921d3bba64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define local file paths\n",
    "train_path = f'/home/sagemaker-user/Product/train_Generic_Client_DS_{ds_ratio}.csv'\n",
    "validation_path = f'/home/sagemaker-user/Product/validation_Generic_Client_DS_{ds_ratio}.csv'\n",
    "test_path = f'/home/sagemaker-user/Product/test_Generic_Client_DS_{ds_ratio}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d6990-8db7-4ea3-8101-654132bb7a7e",
   "metadata": {},
   "source": [
    "##### Run if you want to load original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8a950-9e4e-4506-8771-953364c4e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local file paths\n",
    "train_path = f'/home/sagemaker-user/Product/train_Generic_Client.csv'\n",
    "validation_path = f'/home/sagemaker-user/Product/validation_Generic_Client.csv'\n",
    "test_path = f'/home/sagemaker-user/Product/test_Generic_Client.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d6d58-8c73-40fe-a4e6-697565b7bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_undersampled = pd.read_csv(train_path)\n",
    "validation_df = pd.read_csv(validation_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c97f0-9c40-43be-8cb6-531dc52d8c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define file names\n",
    "train_file = f'{client_name}/train_Generic_Client_DS_{ds_ratio}.csv'\n",
    "validation_file = f'{client_name}/validation_Generic_Client_DS_{ds_ratio}.csv'\n",
    "test_file = f'{client_name}/test_Generic_Client_DS_{ds_ratio}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1ba72-55bc-4842-a2cc-ce100d86b990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload files\n",
    "pipeline.upload_to_s3(train_path, bucket, train_file)\n",
    "pipeline.upload_to_s3(validation_path, bucket, validation_file)\n",
    "pipeline.upload_to_s3(test_path, bucket, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eba5fc-548e-4d10-899b-044fea8c2581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete df to save memory\n",
    "del df\n",
    "\n",
    "# Manually call garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f98ad5-bd0e-46a7-a2cb-eac53eedb7da",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c1af1-3ca2-489a-845b-375d25b70f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define local file paths\n",
    "train_path = f'/home/sagemaker-user/Product/train_Generic_Client_DS_{ds_ratio}.csv'\n",
    "validation_path = f'/home/sagemaker-user/Product/validation_Generic_Client_DS_{ds_ratio}.csv'\n",
    "test_path = f'/home/sagemaker-user/Product/test_Generic_Client_DS_{ds_ratio}.csv'\n",
    "\n",
    "train_identifiers_path = f'/home/sagemaker-user/Product/train_identifiers_Generic_Client_DS_{ds_ratio}.csv'\n",
    "validation_identifiers_path = f'/home/sagemaker-user/Product/validation_identifiers_Generic_Client_DS_{ds_ratio}.csv'\n",
    "test_identifiers_path = f'/home/sagemaker-user/Product/test_identifiers_Generic_Client_DS_{ds_ratio}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15e270-6e81-474b-9bc8-783f445ff5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Loading training, validation and test data...\")\n",
    "train_data = pd.read_csv(train_path)\n",
    "validation_data = pd.read_csv(validation_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f2245-1410-4745-87c8-d8b5f326390c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Defining column types...\")\n",
    "numeric_columns = train_data.drop('MODEL_TARGET', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_columns = train_data.drop('MODEL_TARGET', axis=1).select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(\"Column types defined.\")\n",
    "print(f'Number of numeric columns: {len(numeric_columns)} \\n Numeric columns: {numeric_columns})')\n",
    "print(f'Number of categorical columns: {len(categorical_columns)} \\n categorical columns: {categorical_columns})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3709465-9966-48dc-98f2-627f4e4456f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Debugging step 2: Ensure 'MODEL_TARGET' is not dropped\n",
    "if 'MODEL_TARGET' not in train_data.columns:\n",
    "    raise ValueError(\"'MODEL_TARGET' column not found in training data\")\n",
    "if 'MODEL_TARGET' not in validation_data.columns:\n",
    "    raise ValueError(\"'MODEL_TARGET' column not found in validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a613b3-0134-4f85-9ac5-e9aa95599d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Preparing training data...\")\n",
    "X_train = train_data.drop('MODEL_TARGET', axis=1)\n",
    "y_train = train_data['MODEL_TARGET']\n",
    "print(f\"Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3aa14f-fa7f-40d6-b79d-f28c9664cc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Preparing validation data...\")\n",
    "X_validation = validation_data.drop('MODEL_TARGET', axis=1)\n",
    "y_validation = validation_data['MODEL_TARGET']\n",
    "print(f\"Shape of X_validation: {X_validation.shape}, Shape of y_validation: {y_validation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d762180-f034-4324-bbf6-c84136f5f515",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Preparing test data...\")\n",
    "X_test = test_data.drop('MODEL_TARGET', axis=1)\n",
    "y_test = test_data['MODEL_TARGET']\n",
    "print(f\"Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf876ef-e215-4638-89b5-0a31caa5630c",
   "metadata": {},
   "source": [
    "### Pipeline from model_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12fff9-a8cb-4ac1-af6a-a26b66c29681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train using the preprocessor\n",
    "def preprocess_multi_model(algorithm: str, data: pd.DataFrame, preprocessor: sklearn.compose._column_transformer.ColumnTransformer, partition: str):\n",
    "    if partition == 'train': # then fit the preprocessor to the data before transforming it\n",
    "        logger.info(f'Preprocessing training data for {algorithm}.')   \n",
    "        if algorithm == 'catboost':\n",
    "            # Separate X_train into numerical and categorical data\n",
    "            data_numeric = data[numeric_columns]\n",
    "            data_categorical = data[categorical_columns]\n",
    "\n",
    "            # Access numeric transformer from preprocessor\n",
    "            numeric_transformer = preprocessor.named_transformers_['num']\n",
    "            # Apply preprocessing to numerical data\n",
    "            data_numeric_preprocessed = numeric_transformer.fit_transform(data_numeric)\n",
    "\n",
    "            # Convert categorical data to strings if they are not already\n",
    "            data_categorical = data_categorical.astype(str)\n",
    "\n",
    "            # Combine preprocessed numerical and categorical data\n",
    "            data_combined = np.hstack([data_numeric_preprocessed, data_categorical])\n",
    "\n",
    "            # The categorical features are now the last columns in X_train_combined\n",
    "            # So, return the indices of the features occuring after the numeric features in ascending order \n",
    "            cat_features_indices = list(range(data_numeric_preprocessed.shape[1], data_combined.shape[1]))\n",
    "            logger.info('Data preprocessed.')\n",
    "            return data_combined, cat_features_indices \n",
    "        else:\n",
    "            data_preprocessed = preprocessor.fit_transform(data)\n",
    "            logger.info('Data preprocessed.')\n",
    "            return data_preprocessed, None # Return two variables to ensure consistency with CatBoost option\n",
    "    else: # for validation and test sets, just transform the data\n",
    "        logger.info(f'Preprocessing {partition} data for {algorithm}.') \n",
    "        if algorithm == 'catboost':\n",
    "            # Separate X_train into numerical and categorical data\n",
    "            data_numeric = data[numeric_columns]\n",
    "            data_categorical = data[categorical_columns]\n",
    "\n",
    "            # Access numeric transformer from preprocessor\n",
    "            numeric_transformer = preprocessor.named_transformers_['num']\n",
    "            # Apply preprocessing to numerical data\n",
    "            data_numeric_preprocessed = numeric_transformer.transform(data_numeric)\n",
    "\n",
    "            # Convert categorical data to strings if they are not already\n",
    "            data_categorical = data_categorical.astype(str)\n",
    "\n",
    "            # Combine preprocessed numerical and categorical data\n",
    "            data_combined = np.hstack([data_numeric_preprocessed, data_categorical])\n",
    "\n",
    "            # The categorical features are now the last columns in X_train_combined\n",
    "            # So, return the indices of the features occuring after the numeric features in ascending order \n",
    "            cat_features_indices = list(range(data_numeric_preprocessed.shape[1], data_combined.shape[1]))\n",
    "            logger.info('Data preprocessed.')\n",
    "            return data_combined, cat_features_indices \n",
    "        else:\n",
    "            data_preprocessed = preprocessor.transform(data)\n",
    "            logger.info('Data preprocessed.')\n",
    "            return data_preprocessed, None # Return two variables to ensure consistency with CatBoost option\n",
    "        \n",
    "#print(f\"Shape of X_train_preprocessed: {X_train_preprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72279068-9fdb-4694-a068-78214c5a2310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a5c50-8d06-4f71-a3cf-9dd05f369b14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Defining column types...\")\n",
    "numeric_columns = train_data.drop('MODEL_TARGET', axis=1).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_columns = train_data.drop('MODEL_TARGET', axis=1).select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(\"Column types defined.\")\n",
    "print(f'Number of numeric columns: {len(numeric_columns)} \\n Numeric columns: {numeric_columns})')\n",
    "print(f'Number of categorical columns: {len(categorical_columns)} \\n categorical columns: {categorical_columns})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466786f-2d39-49f9-8780-4e9b2828a584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model pipeline\n",
    "algorithm = 'logistic_regression'  # Change this to try different algorithms\n",
    "use_grid_search = False  # Set True to use GridSearchCV, False for standard pipeline\n",
    "\n",
    "# Generate training pipeline\n",
    "training_pipeline = model_pipeline.create_pipeline(numeric_columns, categorical_columns, algorithm, use_grid_search=use_grid_search)\n",
    "\n",
    "# Extract the preprocessor from the pipeline\n",
    "preprocessor = training_pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Preprocess training data\n",
    "X_train_preprocessed, cat_features_indices = preprocess_multi_model(algorithm, X_train, preprocessor, 'train')\n",
    "\n",
    "# Preprocess validation data\n",
    "X_val_preprocessed, _ = preprocess_multi_model(algorithm, X_validation, preprocessor, 'validation')\n",
    "\n",
    "# Preprocess validation data\n",
    "X_test_preprocessed, _ = preprocess_multi_model(algorithm, X_test, preprocessor, 'test')\n",
    "\n",
    "# Extract the classifier from the pipeline\n",
    "classifier = training_pipeline.named_steps['classifier']\n",
    "#classifier = training_pipeline # for GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20247866-d130-4c78-bdb5-4791f4220306",
   "metadata": {},
   "source": [
    "#### Training and Validation (No Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ea6d4-d48d-4e98-8295-fcb9eedee99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "logger.info(\"Fitting the model...\")\n",
    "if algorithm == 'catboost':\n",
    "    classifier.fit(X_train_preprocessed, y_train, cat_features=cat_features_indices)\n",
    "    model_pipeline.save_model(classifier, algorithm=algorithm, run=1)\n",
    "else:\n",
    "    classifier.fit(X_train_preprocessed, y_train)\n",
    "    model_pipeline.save_model(classifier, algorithm=algorithm, run=1)\n",
    "    \n",
    "# Evaluate the model on the validation set\n",
    "logger.info(\"Making predictions on validation data...\")\n",
    "y_pred_validation = classifier.predict(X_val_preprocessed)\n",
    "\n",
    "# Compute and display metrics\n",
    "accuracy = accuracy_score(y_validation, y_pred_validation)\n",
    "roc_auc = roc_auc_score(y_validation, y_pred_validation)\n",
    "precision = sk_precision_score(y_validation, y_pred_validation)\n",
    "recall = recall_score(y_validation, y_pred_validation)\n",
    "f1 = f1_score(y_validation, y_pred_validation)\n",
    "\n",
    "logger.info(f\"Accuracy: {accuracy}\")\n",
    "logger.info(f\"ROC AUC: {roc_auc}\")\n",
    "logger.info(f\"Precision: {precision}\")\n",
    "logger.info(f\"Recall: {recall}\")\n",
    "logger.info(f\"F1 Score: {f1} \\n\")\n",
    "\n",
    "# Detailed classification report\n",
    "report = classification_report(y_validation, y_pred_validation)\n",
    "print(\"Classification Report for Validation Data:\")\n",
    "print(report)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "# sklearn confusion matrix layout:\n",
    "# TN  FP\n",
    "# FN  TP\n",
    "cm = confusion_matrix(y_validation, y_pred_validation)\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_validation, y_pred_validation)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Compute Precision-Recall curve and average precision\n",
    "# Precision-Recall curve robust with imbalanced datasets!\n",
    "precision, recall, _ = precision_recall_curve(y_validation, classifier.predict(X_val_preprocessed))\n",
    "average_precision = average_precision_score(y_validation, classifier.predict(X_val_preprocessed))\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(f'Precision-Recall curve: AP={average_precision:0.2f}')\n",
    "plt.show()\n",
    "\n",
    "logger.info(f\"Average Precision: {average_precision:.2f}\")\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "logger.info(\"Model training and evaluation completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc6eac-1782-49f7-a82a-7247f19a1f3e",
   "metadata": {},
   "source": [
    "#### Training and Validation (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5f392-47b1-4e54-9460-6d4fd343b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for each model\n",
    "algorithm = 'logistic_regression'\n",
    "\n",
    "# For classifiers with \"scale_pos_weight\" argument, compute \"scale_pos_weight\".\n",
    "# This argument helps the model handle class imbalances by giving more weight to fraud cases.\n",
    "neg = (y_train == 0).sum()\n",
    "pos = (y_train == 1).sum()\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": [1500],\n",
    "        \"max_depth\": [10, 20],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2]\n",
    "    },\n",
    "    \"xgb\": {\n",
    "        \"max_depth\": [6, 10],\n",
    "        \"learning_rate\": [0.01, 0.05],\n",
    "        \"n_estimators\": [1500],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "        \"scale_pos_weight\": [scale_pos_weight]\n",
    "    },\n",
    "    \"catboost\": {\n",
    "        \"depth\": [6, 10],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"iterations\": [1000, 1200]\n",
    "    },\n",
    "    \"logistic_regression\": {\n",
    "        \"C\": [0.01, 0.1, 0.5, 1.0, 5.0],\n",
    "        \"solver\": [\"liblinear\", \"lbfgs\", \"saga\"],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"class_weight\": [\"balanced\"],\n",
    "        \"max_iter\": [200, 500]\n",
    "    },\n",
    "    \"svm\": {\n",
    "        \"C\": [0.1, 1.0],\n",
    "        \"kernel\": [\"rbf\"],\n",
    "        \"gamma\": [\"scale\"],\n",
    "        \"max_iter\": [1000, 2000]\n",
    "    },\n",
    "    \"neural_network\": {\n",
    "        \"hidden_layer_sizes\": [(256, 128), (128, 64)],\n",
    "        \"alpha\": [0.0001, 0.001],\n",
    "        \"max_iter\": [500, 1000]\n",
    "    },\n",
    "    \"light_gbm\": {\n",
    "        \"objective\": [\"binary\"],\n",
    "        \"num_leaves\": [31],               # Controls complexity\n",
    "        \"max_depth\": [10, 20],            # -1 means no limit\n",
    "        \"learning_rate\": [0.01, 0.05],   # Typical values\n",
    "        \"n_estimators\": [1500],          # # of boosting rounds\n",
    "        \"min_child_samples\": [20, 40],        # Minimum data in a leaf\n",
    "        \"subsample\": [0.8, 1.0],              # Row sampling (bagging)\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "        \"scale_pos_weight\": [scale_pos_weight]\n",
    "        #\"reg_alpha\": [0.0, 0.1],              # L1 regularization\n",
    "        #\"reg_lambda\": [0.0, 0.1]              # L2 regularization\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get hyperparameter combinations for selected model\n",
    "if algorithm in hyperparameter_grid:\n",
    "    param_combinations = list(product(*hyperparameter_grid[algorithm].values()))\n",
    "    param_names = list(hyperparameter_grid[algorithm].keys())\n",
    "else:\n",
    "    raise ValueError(f\"No hyperparameter grid found for {algorithm}\")\n",
    "\n",
    "# Track best model performance\n",
    "best_model = None\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "# Iterate over all hyperparameter combinations\n",
    "for param_values in param_combinations:\n",
    "    params = dict(zip(param_names, param_values))\n",
    "    logger.info(f\"Training with parameters: {params}\")\n",
    "\n",
    "    # Instantiate model with current hyperparameters\n",
    "    if algorithm == 'random_forest':\n",
    "        classifier = model_pipeline.CustomRandomForest(**params)\n",
    "    elif algorithm == 'xgb':\n",
    "        classifier = xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "    elif algorithm == 'catboost':\n",
    "        classifier = cb.CatBoostClassifier(**params, verbose=False, random_state=42)\n",
    "    elif algorithm == 'logistic_regression':\n",
    "        classifier = model_pipeline.CustomLogisticRegression(**params)\n",
    "    elif algorithm == 'svm':\n",
    "        classifier = model_pipeline.CustomSVM(**params)\n",
    "    elif algorithm == 'neural_network':\n",
    "        classifier = model_pipeline.CustomNeuralNetwork(**params)\n",
    "    elif algorithm == 'light_gbm':\n",
    "        classifier = LGBMClassifier(**params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n",
    "\n",
    "    # Fit the model\n",
    "    logger.info(\"Fitting the model...\")\n",
    "    if algorithm == 'catboost':\n",
    "        classifier.fit(X_train_preprocessed, y_train, cat_features=cat_features_indices)\n",
    "    else:\n",
    "        classifier.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    logger.info(\"Making predictions on validation data...\")\n",
    "    y_pred_validation = classifier.predict(X_val_preprocessed)\n",
    "    y_pred_proba = classifier.predict_proba(X_val_preprocessed)[:, 1]  # Use probabilities for AUC-ROC\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_validation, y_pred_validation)\n",
    "    roc_auc = roc_auc_score(y_validation, y_pred_proba)\n",
    "    precision = sk_precision_score(y_validation, y_pred_validation)\n",
    "    recall = recall_score(y_validation, y_pred_validation)\n",
    "    f1 = f1_score(y_validation, y_pred_validation)\n",
    "\n",
    "    logger.info(f\"Results for {params} - Accuracy: {accuracy}, ROC AUC: {roc_auc}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "\n",
    "    # Track best model using ROC-AUC\n",
    "    if roc_auc > best_score:\n",
    "        best_score = roc_auc\n",
    "        best_model = classifier\n",
    "        best_params = params\n",
    "\n",
    "# Save best model\n",
    "logger.info(f\"Best parameters found: {best_params}\")\n",
    "model_pipeline.save_model(best_model, algorithm=algorithm, run=1)\n",
    "\n",
    "# Evaluate best model on validation set\n",
    "logger.info(\"Evaluating best model on validation set...\")\n",
    "y_pred_validation = best_model.predict(X_val_preprocessed)\n",
    "y_pred_proba = best_model.predict_proba(X_val_preprocessed)[:, 1]\n",
    "\n",
    "# Compute and display metrics for best model\n",
    "accuracy = accuracy_score(y_validation, y_pred_validation)\n",
    "roc_auc = roc_auc_score(y_validation, y_pred_proba)\n",
    "precision = sk_precision_score(y_validation, y_pred_validation)\n",
    "recall = recall_score(y_validation, y_pred_validation)\n",
    "f1 = f1_score(y_validation, y_pred_validation)\n",
    "\n",
    "logger.info(f\"Best Model - Accuracy: {accuracy}, ROC AUC: {roc_auc}, Precision: {precision}, Recall: {recall}, F1 Score: {f1} \\n\")\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_validation, y_pred_validation)\n",
    "print(\"Classification Report for Validation Data:\")\n",
    "print(report)\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_validation, y_pred_validation)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute and plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_validation, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_validation, y_pred_proba)\n",
    "average_precision = average_precision_score(y_validation, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(f'Precision-Recall curve: AP={average_precision:0.2f}')\n",
    "plt.show()\n",
    "\n",
    "logger.info(f\"Average Precision: {average_precision:.2f}\")\n",
    "logger.info(\"Model training and evaluation completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354a519-9fab-4fd1-a061-24af87a38b7b",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61b4e4-24e2-4137-b59a-505f54db2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the 'models' folder\n",
    "models_folder = '/home/sagemaker-user/Product'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcf85e-01b7-4e6b-9965-dc0f379be836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through the model files in the 'models' folder\n",
    "for model_file in os.listdir(models_folder):\n",
    "    if model_file.endswith('model_logistic_regression_1_2025-04-22 09:58:38.pkl'):\n",
    "        model_path = os.path.join(models_folder, model_file)\n",
    "        if algorithm == 'catboost':\n",
    "            # Load the CatBoost model\n",
    "            catboost_model = cb.CatBoostClassifier()\n",
    "            catboost_model.load_model(model_path)\n",
    "            # Make predictions on the test data\n",
    "            y_pred_prob_test = catboost_model.predict_proba(X_test_preprocessed)\n",
    "            # y_pred_prob_test will have two columns: one for the probability of class 0, and the other for class 1\n",
    "            # To get the probability of class 1, select the second column\n",
    "            y_pred_test = y_pred_prob_test[:, 1]\n",
    "        \n",
    "        if algorithm == 'xgb':\n",
    "            # Load the model\n",
    "            bst = xgb.Booster(model_file=model_path)\n",
    "            # Make predictions on the test data\n",
    "            y_pred_test = bst.predict(xgb.DMatrix(X_test_preprocessed))\n",
    "            \n",
    "        else: # has to be an sklearn model, saved using pickle\n",
    "            model = model_pipeline.load_model(model_path, algorithm)\n",
    "            # Make predictions\n",
    "            y_pred_test = model_pipeline.make_predictions(model, algorithm, X_test_preprocessed)\n",
    "\n",
    "        # Convert the predictions to binary values (0 or 1)\n",
    "        y_pred_rounded = np.round(y_pred_test)\n",
    "\n",
    "        print(f\"Model: {model_file}\")\n",
    "        \n",
    "        # Compute and display metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred_rounded)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_rounded)\n",
    "        precision = sk_precision_score(y_test, y_pred_rounded)\n",
    "        recall = recall_score(y_test, y_pred_rounded)\n",
    "        f1 = f1_score(y_test, y_pred_rounded)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"ROC AUC: {roc_auc}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        \n",
    "        # Get the classification report\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred_rounded))\n",
    "        \n",
    "        # Generate the confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred_rounded)\n",
    "\n",
    "        # Display the confusion matrix\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Compute ROC curve and ROC area\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_rounded)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Plot the ROC curve\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Convert the predictions array into a DataFrame\n",
    "        df_preds = pd.DataFrame(y_pred_test, columns=['Prediction'])\n",
    "        \n",
    "        # Join predictions to enquiry keys\n",
    "        # Enquiry keys, model target and tvh partition contained in test_identifiers_df\n",
    "        test_identifiers_path = f'/home/sagemaker-user/Product/test_identifiers_Generic_Client_DS_{ds_ratio}.csv'\n",
    "        test_identifiers_df = pd.read_csv(test_identifiers_path)\n",
    "\n",
    "        # Saving the combined DataFrame\n",
    "        results_df.to_csv(f'/home/sagemaker-user/Product/Generic_Client_Output_{model_file}.csv', index=False)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbf673-309e-4fea-8c7c-cc4dd5a1b7ce",
   "metadata": {},
   "source": [
    "### Logistic Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8121d5-966c-4bf0-841f-7bd6237791c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names for preprocessed features\n",
    "feature_names = get_transformed_feature_names(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07701a99-4932-453e-b5df-60ab69943196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients from best logistic regression model\n",
    "coefficients = model.model.coef_[0]  # shape: (n_features,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8deb844-5b05-4be7-b16a-31b640401424",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b393a8b-6e8b-4ca6-ac3f-d8926c686e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of feature importance\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": coefficients,\n",
    "    \"AbsCoefficient\": np.abs(coefficients)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c640dbe-08d6-4873-908c-55c97bd05af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by absolute value\n",
    "coef_df_sorted = coef_df.sort_values(by=\"AbsCoefficient\", ascending=False)\n",
    "coef_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ced99d-8458-429c-887f-ff0c77ade7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20  # Show top 20 features\n",
    "top_features = coef_df_sorted.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(top_features[\"Feature\"], top_features[\"Coefficient\"])\n",
    "plt.axvline(x=0, color='black', linestyle='--')\n",
    "plt.title(\"Top 20 Logistic Regression Coefficients (Impact on Fraud Likelihood)\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b8881-f543-44cd-917a-e5a7b85ed78c",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d0b03-3363-4a9d-9d72-05873d083745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_feature_names(column_transformer):\n",
    "    \"\"\"\n",
    "    Extract transformed feature names from a fitted ColumnTransformer.\n",
    "    Handles pipelines and transformers with get_feature_names_out.\n",
    "\n",
    "    Args:\n",
    "        column_transformer (ColumnTransformer): Fitted ColumnTransformer.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of transformed feature names.\n",
    "    \"\"\"\n",
    "    feature_names = []\n",
    "\n",
    "    for name, transformer, original_features in column_transformer.transformers_:\n",
    "        if name == 'remainder' and transformer == 'passthrough':\n",
    "            feature_names.extend(original_features)\n",
    "            continue\n",
    "\n",
    "        # Check if it's a pipeline\n",
    "        if isinstance(transformer, Pipeline):\n",
    "            # Iterate steps in reverse to find the first with get_feature_names_out\n",
    "            for step_name, step_transformer in reversed(transformer.steps):\n",
    "                if hasattr(step_transformer, 'get_feature_names_out'):\n",
    "                    if isinstance(step_transformer, OneHotEncoder):\n",
    "                        names = step_transformer.get_feature_names_out(original_features)\n",
    "                    else:\n",
    "                        names = step_transformer.get_feature_names_out()\n",
    "                    feature_names.extend(names)\n",
    "                    break\n",
    "            else:\n",
    "                # No transformer in pipeline supports get_feature_names_out\n",
    "                feature_names.extend(original_features)\n",
    "        elif hasattr(transformer, 'get_feature_names_out'):\n",
    "            # Transformer supports get_feature_names_out directly\n",
    "            feature_names.extend(transformer.get_feature_names_out(original_features))\n",
    "        else:\n",
    "            # Fallback to original feature names\n",
    "            feature_names.extend(original_features)\n",
    "\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1f4b9-df65-4a38-92aa-dd145e79d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names for preprocessed features\n",
    "feature_names = get_transformed_feature_names(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d79c58-b253-439e-bc5d-1feaaba91582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SHAP works best on raw, unscaled features ---\n",
    "# Use X_train_preprocessed *only if* preprocessing is minimal\n",
    "# Otherwise use original features: X_train, X_validation\n",
    "\n",
    "# As X_val_preprocessed is numpy array, construct a DataFrame with correct column names and index\n",
    "X_explain = pd.DataFrame(X_val_preprocessed, columns=feature_names, index=X_validation.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ca729-9496-4ebb-a83e-cc07b99c8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_explain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cecc52-7b60-46a3-a405-99954db792ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(bst, X_explain) # change 'bst' to be model agnostic\n",
    "# Compute SHAP values\n",
    "shap_values = explainer(X_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc73c5c-67a8-48ec-8e07-6abda0d04668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Global Summary Plot ---\n",
    "shap.summary_plot(shap_values, X_explain, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fb1dc-c6b8-4bb6-9cb6-4802a9d05d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this when you want to rank features by importance (average impact)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42830a13-18c6-43e2-afef-6e84c3a3905b",
   "metadata": {},
   "source": [
    "#### Explain One Fraud Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64698556-ef0c-42e1-ae68-1ff797779d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one fraud case\n",
    "fraud_index = y_validation[y_validation == 1].index[0]\n",
    "\n",
    "# Show feature contributions using a waterfall plot (no JavaScript)\n",
    "shap.plots.waterfall(shap_values[fraud_index])"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
